{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.1-10-gc2403eb torch 1.7.0 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (8 CPUs, 11.4 GB RAM, 76.9/95.6 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataset_folder = \"/home/sara/Desktop/Master-thesis/dataset\"\n",
    "yolo_folder = \"/home/sara/Desktop/Master-thesis/yolov5\"\n",
    "\n",
    "%cd {yolo_folder}\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "\n",
    "\n",
    "from yolov5 import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sara/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-2-14 torch 1.7.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Saved 1 image to \u001b[1m/home/sara/Desktop/Master-thesis/master_thesis/Dataset/yolo_results/untrained_weights\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5n')  # or yolov5m, yolov5l, yolov5x, custom\n",
    "\n",
    "# Images\n",
    "img = '{dataset_folder}/images_one/0a0df8299.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n",
    "\n",
    "# Inference\n",
    "results = model(img)\n",
    "\n",
    "# Results\n",
    "torch.save(model, \"/home/sara/Desktop/Master-thesis/master_thesis/Dataset/yolov5n.pt\")\n",
    "results.save(\"/home/sara/Desktop/Master-thesis/master_thesis/Dataset/yolo_results/untrained_weights/\")  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sara/Desktop/Master-thesis/yolov5\n",
      "usage: detect.py [-h] [--weights WEIGHTS [WEIGHTS ...]] [--source SOURCE]\n",
      "                 [--data DATA] [--imgsz IMGSZ [IMGSZ ...]]\n",
      "                 [--conf-thres CONF_THRES] [--iou-thres IOU_THRES]\n",
      "                 [--max-det MAX_DET] [--device DEVICE] [--view-img]\n",
      "                 [--save-txt] [--save-conf] [--save-crop] [--nosave]\n",
      "                 [--classes CLASSES [CLASSES ...]] [--agnostic-nms]\n",
      "                 [--augment] [--visualize] [--update] [--project PROJECT]\n",
      "                 [--name NAME] [--exist-ok] [--line-thickness LINE_THICKNESS]\n",
      "                 [--hide-labels] [--hide-conf] [--half] [--dnn]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --weights WEIGHTS [WEIGHTS ...]\n",
      "                        model path(s)\n",
      "  --source SOURCE       file/dir/URL/glob, 0 for webcam\n",
      "  --data DATA           (optional) dataset.yaml path\n",
      "  --imgsz IMGSZ [IMGSZ ...], --img IMGSZ [IMGSZ ...], --img-size IMGSZ [IMGSZ ...]\n",
      "                        inference size h,w\n",
      "  --conf-thres CONF_THRES\n",
      "                        confidence threshold\n",
      "  --iou-thres IOU_THRES\n",
      "                        NMS IoU threshold\n",
      "  --max-det MAX_DET     maximum detections per image\n",
      "  --device DEVICE       cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
      "  --view-img            show results\n",
      "  --save-txt            save results to *.txt\n",
      "  --save-conf           save confidences in --save-txt labels\n",
      "  --save-crop           save cropped prediction boxes\n",
      "  --nosave              do not save images/videos\n",
      "  --classes CLASSES [CLASSES ...]\n",
      "                        filter by class: --classes 0, or --classes 0 2 3\n",
      "  --agnostic-nms        class-agnostic NMS\n",
      "  --augment             augmented inference\n",
      "  --visualize           visualize features\n",
      "  --update              update all models\n",
      "  --project PROJECT     save results to project/name\n",
      "  --name NAME           save results to project/name\n",
      "  --exist-ok            existing project/name ok, do not increment\n",
      "  --line-thickness LINE_THICKNESS\n",
      "                        bounding box thickness (pixels)\n",
      "  --hide-labels         hide labels\n",
      "  --hide-conf           hide confidences\n",
      "  --half                use FP16 half-precision inference\n",
      "  --dnn                 use OpenCV DNN for ONNX inference\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd {yolo_folder}\n",
    "!python yolov5/detect.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ebara/Documents/master_thesis\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/home/ebara/Documents/master_thesis/yolo_scripts/yolov5n.pt'], source=/media/ebara/46CC6137CC612303/test_v2, data=yolo_scripts/yolov5/data/coco128.yaml, imgsz=[768, 768], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=Dataset/yolo_results/untrained_weights, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "/home/ebara/miniconda3/envs/yolo/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "YOLOv5 ðŸš€ v6.0-276-ga936f5f torch 1.7.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "image 1/15606 /media/ebara/46CC6137CC612303/test_v2/00002bd58.jpg: 768x768 Done. (0.067s)\n",
      "image 2/15606 /media/ebara/46CC6137CC612303/test_v2/00015efb6.jpg: 768x768 Done. (0.071s)\n",
      "image 3/15606 /media/ebara/46CC6137CC612303/test_v2/00023d5fc.jpg: 768x768 Done. (0.071s)\n",
      "image 4/15606 /media/ebara/46CC6137CC612303/test_v2/000367c13.jpg: 768x768 Done. (0.063s)\n",
      "image 5/15606 /media/ebara/46CC6137CC612303/test_v2/0008ca6e9.jpg: 768x768 Done. (0.064s)\n",
      "image 6/15606 /media/ebara/46CC6137CC612303/test_v2/000a4635f.jpg: 768x768 Done. (0.070s)\n",
      "image 7/15606 /media/ebara/46CC6137CC612303/test_v2/000bd9dbf.jpg: 768x768 Done. (0.070s)\n",
      "image 8/15606 /media/ebara/46CC6137CC612303/test_v2/000f7d875.jpg: 768x768 Done. (0.076s)\n",
      "image 9/15606 /media/ebara/46CC6137CC612303/test_v2/0010551d9.jpg: 768x768 1 remote, Done. (0.073s)\n",
      "image 10/15606 /media/ebara/46CC6137CC612303/test_v2/001839c6f.jpg: 768x768 Done. (0.076s)\n",
      "image 11/15606 /media/ebara/46CC6137CC612303/test_v2/002a943bf.jpg: 768x768 Done. (0.070s)\n",
      "image 12/15606 /media/ebara/46CC6137CC612303/test_v2/00313b166.jpg: 768x768 Done. (0.062s)\n",
      "image 13/15606 /media/ebara/46CC6137CC612303/test_v2/00327b02d.jpg: 768x768 Done. (0.066s)\n",
      "image 14/15606 /media/ebara/46CC6137CC612303/test_v2/0035268d9.jpg: 768x768 Done. (0.071s)\n",
      "image 15/15606 /media/ebara/46CC6137CC612303/test_v2/003b58a76.jpg: 768x768 Done. (0.071s)\n",
      "image 16/15606 /media/ebara/46CC6137CC612303/test_v2/0044e3dda.jpg: 768x768 Done. (0.076s)\n",
      "image 17/15606 /media/ebara/46CC6137CC612303/test_v2/0046967a4.jpg: 768x768 Done. (0.072s)\n",
      "image 18/15606 /media/ebara/46CC6137CC612303/test_v2/004751507.jpg: 768x768 Done. (0.071s)\n",
      "image 19/15606 /media/ebara/46CC6137CC612303/test_v2/0047c79fc.jpg: 768x768 Done. (0.074s)\n",
      "image 20/15606 /media/ebara/46CC6137CC612303/test_v2/004946ceb.jpg: 768x768 Done. (0.072s)\n",
      "image 21/15606 /media/ebara/46CC6137CC612303/test_v2/004dd347a.jpg: 768x768 Done. (0.063s)\n",
      "image 22/15606 /media/ebara/46CC6137CC612303/test_v2/00506a19e.jpg: 768x768 Done. (0.067s)\n",
      "image 23/15606 /media/ebara/46CC6137CC612303/test_v2/0057c059b.jpg: 768x768 Done. (0.068s)\n",
      "image 24/15606 /media/ebara/46CC6137CC612303/test_v2/005b8152d.jpg: 768x768 Done. (0.064s)\n",
      "image 25/15606 /media/ebara/46CC6137CC612303/test_v2/0063cb1e9.jpg: 768x768 Done. (0.066s)\n",
      "image 26/15606 /media/ebara/46CC6137CC612303/test_v2/00696ecc2.jpg: 768x768 Done. (0.064s)\n",
      "image 27/15606 /media/ebara/46CC6137CC612303/test_v2/006ba5b3b.jpg: 768x768 Done. (0.071s)\n",
      "image 28/15606 /media/ebara/46CC6137CC612303/test_v2/007211988.jpg: 768x768 Done. (0.064s)\n",
      "image 29/15606 /media/ebara/46CC6137CC612303/test_v2/007545acc.jpg: 768x768 Done. (0.071s)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ebara/Documents/master_thesis/yolo_scripts/yolov5/detect.py\", line 257, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/ebara/Documents/master_thesis/yolo_scripts/yolov5/detect.py\", line 252, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/home/ebara/miniconda3/envs/yolo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ebara/Documents/master_thesis/yolo_scripts/yolov5/detect.py\", line 115, in run\n",
      "    for path, im, im0s, vid_cap, s in dataset:\n",
      "  File \"/home/ebara/Documents/master_thesis/yolo_scripts/yolov5/utils/datasets.py\", line 219, in __next__\n",
      "    img0 = cv2.imread(path)  # BGR\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#from PIL import Image\n",
    "%cd {yolo_folder}\n",
    "\n",
    "test_dataset = f\"{dataset_folder}/test_v2\"\n",
    "save_folder = \"Dataset/yolo_results/untrained_weights\"\n",
    "\n",
    "!python {yolo_folder}yolov5/detect.py --weights {yolo_folder}yolov5n.pt --img 768 --source {test_dataset} --project {save_folder}\n",
    "#display.Image(filename='Dataset/images_one/0a0df8299.jpg', width=600)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183b0f8ce73dacaa42f1974bca33c4be1e696327158103aff23c1635ce5bf913"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
